===============================================
 Simple chaotic behavior in non-linear systems
===============================================

Consider the seemingly trivial problem of evaluating with a computer the
expression

.. math::

   f(x) = r x (1-x)

where :math:`r` and :math:`x` are real numbers with :math:`r \in [0,4]` and
:math:`x \in (0,1)`.  This expression can also be written in an algebraically
equivalent form:

.. math::

   f_2(x) = rx - rx^2.

We will see, however, that when using a computer these two forms don't
necessarily produce the same answer.  Computers can not represent the real
numbers (a mathematical abstraction with infinite precision) but instead must
use finite-precision numbers that can fit in finite memory.  The two
expressions above can, therefore, lead to slightly different answers as the
various (algebraically equivalent) operations are carried by the computer.

First a look at a few simple tests::

    In [1]: def f1(x): return r*x*(1-x)
       ...: def f2(x): return r*x - r*x**2

    In [2]: r=1.9
       ...: x=0.8
       ...: print 'f1:', f1(x)
       ...: print 'f2:', f2(x)
    f1: 0.304
    f2: 0.304

    In [3]: r=3.9
       ...: x=0.8
       ...: print 'f1:', f1(x)
       ...: print 'f2:', f2(x)
    f1: 0.624
    f2: 0.624

At three digits, no difference is visible.  But if we print more digits, we see
the results aren't quite identical::

    In [5]: print 'difference: %.16e' % (f1(x)-f2(x))
    difference: 2.2204460492503131e-16

More importantly, this difference begins to accumulate as we perform the same
operations over and over.  Let's illustrate this behavior by using the formulas
above *iteratively*, that is, by feeding the result of the evaluation back into
the same formula:

.. math::

   x_{n+1} = f(x_n), n=0,1, \ldots

We can experiment with different values of :math:`r` and different starting
points :math:`x_0` to observe the different results.  We will simply build a
python list that contains the results of three different (algebraically
equivalent) forms of evaluating the above expression.

.. admonition:: Exercise

   Build a little script that computes the iteration of :math:`f(x)` using
   three different ways of writing the expression.  Store your results and plot
   them using the ``plt.plot()`` function (the solution follows).

For completeness, we define three algebraically equivalent formulations::

    def f1(x): return r*x*(1-x)
    def f2(x): return r*x - r*x**2
    def f3(x): return r*(x-x**2)

In order to see the difference between the initial behavior and the later
evolution,  let's declare two variables to control our plotting::

    num_points = 100  # total number of points to compute
    drop_points = 0  # don't display the first drop_points
    
Using these, a simple script like this::

    x0 = 0.55 # Any starting value
    r  = 3.9  # Change this to see changes in behavior
    fp = (r-1.0)/r
    x1 = x2 = x3 = x0
    data = []
    data.append([x1,x2,x3])
    for i in range(num_points):
	x1 = f1(x1)
	x2 = f2(x2)
	x3 = f3(x3)
	data.append([x1,x2,x3])

    # Display the results
    plt.figure()
    plt.title('r=%1.1f' % r)
    plt.axhline(fp, color='black')
    plt.plot(data[drop_points:], '-o', markersize=4)
    plt.show()

Will produce this figure:

.. plot:: ../book/examples/numerical_chaos.py

.. admonition:: Exercise

   Now, experiment with different values of :math:`r` as well as different
   starting points :math:`x_0`.  What do you see?  What happens when :math:`r`
   is small (close to 0)? Experiment with these values of :math:`r`: [1.9, 2.9,
   3.1, 3.5, 3.9] and think about the behavior of the system as you change
   :math:`r`.

Once we've understood the basic pattern, let's try to think of the entire
evolution of the system *as a function of* :math:`r`.  First, observe that a
sequence generated by an iterative process of the form

.. math::

   x_{n+1} = f(x_n), n=0,1, \ldots

will stop producing new values if there is a certain :math:`x^*` such that

.. math::

   x^* = f(x^*).

This special :math:`x^*` is called a *fixed point* of the iterative process.
It is easy to show that for our :math:`f(x)`, the fixed point is

.. math::

   x^* = \frac{r-1}{r}

(in fact, that's the value plotted as a thin black line in the earlier
script).

.. admonition:: Exercise

   Study whether the iteration converges to the fixed point or not by letting
   it run for each value of r for a few hundred points and discarding those,
   and then plotting the rest.  Make a diagram with these plots as a function
   of r.

The following code is a simple solution:

.. plot::

    def f(x, r):
        return r*x*(1-x)
	
    num_points = 250
    drop_points = 50
    rmin, rmax = 0, 4
    xmin, xmax = 0, 1
    x0 = 0.65
    fig, ax = plt.subplots()
    ax.set_xlim(rmin, rmax)
    ax.set_ylim(xmin, xmax)
    for r in np.linspace(rmin, rmax, 300):
        x = np.empty(num_points)
        x[0] = x0
        for n in range(1, num_points):
            x[n] = f(x[n-1], r)
        x = x[drop_points:]
        rplot = r*np.ones_like(x)
        ax.plot(rplot, x, 'b,')

.. admonition:: Exercise

   Can you relate the features of this figure to the behavior you saw in your
   earlier plots?  Zoom in the region past :math:`r=3`, what finer features do
   you see?  Where is the fixed point we discussed earlier?

   